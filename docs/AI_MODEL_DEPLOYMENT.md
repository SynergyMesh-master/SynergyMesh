# ğŸ¤– AI æ¨¡å‹éƒ¨ç½²æŒ‡å— | AI Model Deployment Guide

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0  
**æœ€åæ›´æ–°**: 2025-12-14  
**é€‚ç”¨ç³»ç»Ÿ**: Unmanned Island System / SynergyMesh Platform

---

## ğŸ“‹ ç›®å½• | Table of Contents

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [ç¡¬ä»¶è¦æ±‚](#ç¡¬ä»¶è¦æ±‚)
3. [æ¨èé…ç½®](#æ¨èé…ç½®)
4. [vLLM éƒ¨ç½²](#vllm-éƒ¨ç½²)
5. [æ¨¡å‹é…ç½®](#æ¨¡å‹é…ç½®)
6. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
7. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜ AI æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯å¤§è¯­è¨€æ¨¡å‹ LLMï¼‰åœ¨ Unmanned Island System ä¸­çš„éƒ¨ç½²è¦æ±‚å’Œæœ€ä½³å®è·µã€‚

### æ”¯æŒçš„æ¨¡å‹ç±»å‹

- **å¤§è¯­è¨€æ¨¡å‹ (LLM)**: GPT, LLaMA, Qwen ç­‰
- **åµŒå…¥æ¨¡å‹ (Embedding)**: å‘é‡åŒ–å’Œè¯­ä¹‰æœç´¢
- **ä»£ç ç”Ÿæˆæ¨¡å‹**: CodeLLaMA, StarCoder ç­‰
- **å¤šæ¨¡æ€æ¨¡å‹**: Vision-Language æ¨¡å‹

---

## ğŸ’» ç¡¬ä»¶è¦æ±‚

### âš ï¸ æœ€ä½è¿è¡Œé…ç½®

**GPU è¦æ±‚**:

- **æ˜¾å­˜**: 24GB
- **æ¨èå¡å‹**: NVIDIA RTX 4090, RTX 3090
- **é™åˆ¶**: ä»…èƒ½æ‹‰èµ·æ¨¡å‹ï¼Œæ— æ³•è¾¾åˆ°å®Œæ•´é•¿åº¦

**é…ç½®è¯´æ˜**:

```yaml
# æœ€ä½é…ç½®é™åˆ¶
max_model_len: 14000  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼š12500-14000
gpu_memory_utilization: 0.90  # GPU æ˜¾å­˜ä½¿ç”¨ç‡
tensor_parallel_size: 1  # å•å¡éƒ¨ç½²
```

**æ€§èƒ½ç‰¹ç‚¹**:

- âœ… å¯ä»¥å¯åŠ¨å’Œè¿è¡Œæ¨¡å‹
- âš ï¸ ä¸Šä¸‹æ–‡é•¿åº¦å—é™ï¼ˆéœ€è¦è®¾ç½® max_lenï¼‰
- âš ï¸ å¯èƒ½å‡ºç° OOMï¼ˆå†…å­˜ä¸è¶³ï¼‰
- âš ï¸ ä¸æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ

### âœ… æ¨èè¿è¡Œé…ç½®

**GPU è¦æ±‚**:

- **æ˜¾å­˜**: 30GB æˆ–ä»¥ä¸Š
- **æ¨èå¡å‹**:
  - NVIDIA A100 (40GB/80GB)
  - NVIDIA H100 (80GB)
  - NVIDIA H800
- **ä¼˜åŠ¿**: èƒ½å®Œæ•´éƒ¨ç½²æ¨¡å‹ä»¥åŠå®Œæ•´é•¿åº¦ä¸Šä¸‹æ–‡

**é…ç½®è¯´æ˜**:

```yaml
# æ¨èé…ç½®
max_model_len: 32768  # å®Œæ•´ä¸Šä¸‹æ–‡é•¿åº¦
gpu_memory_utilization: 0.95  # é«˜ GPU æ˜¾å­˜ä½¿ç”¨ç‡
tensor_parallel_size: 1  # å•å¡æˆ–å¤šå¡
enable_prefix_caching: true  # å¯ç”¨å‰ç¼€ç¼“å­˜
```

**æ€§èƒ½ç‰¹ç‚¹**:

- âœ… å®Œæ•´ä¸Šä¸‹æ–‡é•¿åº¦æ”¯æŒ
- âœ… ç¨³å®šçš„æ¨ç†æ€§èƒ½
- âœ… é€‚åˆç”Ÿäº§ç¯å¢ƒ
- âœ… æ”¯æŒæ‰¹å¤„ç†å’Œå¹¶å‘è¯·æ±‚

### ğŸ“Š é…ç½®å¯¹æ¯”è¡¨

| é…ç½®é¡¹ | æœ€ä½é…ç½® (24GB) | æ¨èé…ç½® (30GB+) |
|--------|----------------|------------------|
| GPU å‹å· | RTX 4090 | A100/H100 |
| æ˜¾å­˜ | 24GB | 30GB-80GB |
| ä¸Šä¸‹æ–‡é•¿åº¦ | 12500-14000 | 32768+ |
| æ‰¹å¤„ç†å¤§å° | 1-4 | 8-32 |
| å¹¶å‘è¯·æ±‚ | å—é™ | é«˜ |
| ç”Ÿäº§å°±ç»ª | âŒ | âœ… |

---

## ğŸš€ æ¨èé…ç½®

### æ“ä½œç³»ç»Ÿé€‰æ‹©

**å¼ºçƒˆæ¨è**:

- **Ubuntu 22.04 LTS** âœ…
- **Ubuntu 20.04 LTS** âœ…
- **å…¶ä»– Linux å¼€æºç‰ˆæœ¬** (Debian, CentOS Stream, Rocky Linux) âœ…

**åŸå› **:

1. å®Œæ•´æ”¯æŒ vLLM
2. CUDA é©±åŠ¨å…¼å®¹æ€§å¥½
3. å®¹å™¨åŒ–éƒ¨ç½²ç¨³å®š
4. ç¤¾åŒºæ”¯æŒå®Œå–„

**ä¸æ¨è**:

- âŒ Windows (WSL2 å¯ç”¨ä½†æ€§èƒ½å—é™)
- âŒ macOS (æ—  NVIDIA GPU æ”¯æŒ)

### CUDA å’Œé©±åŠ¨è¦æ±‚

```bash
# æ£€æŸ¥ NVIDIA é©±åŠ¨
nvidia-smi

# è¦æ±‚ç‰ˆæœ¬
CUDA: >= 11.8
Driver: >= 520.00
```

---

## ğŸ³ vLLM éƒ¨ç½²

### æ–¹æ³• 1: Docker éƒ¨ç½²ï¼ˆæ¨èï¼‰

#### æ­¥éª¤ 1: æ‹‰å–å®˜æ–¹é•œåƒ

```bash
# æ‹‰å– vLLM OpenAI å…¼å®¹æœåŠ¡å™¨é•œåƒ
docker pull vllm/vllm-openai:v0.12.0
```

#### æ­¥éª¤ 2: å¯åŠ¨å®¹å™¨

```bash
# åŸºç¡€å¯åŠ¨ï¼ˆæœ€ä½é…ç½®ï¼‰
docker run --gpus all \
  --name vllm-server \
  -p 8000:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  vllm/vllm-openai:v0.12.0 \
  --model Qwen/Qwen2.5-7B-Instruct \
  --max-model-len 14000 \
  --gpu-memory-utilization 0.90

# æ¨èé…ç½®å¯åŠ¨
docker run --gpus all \
  --name vllm-server \
  -p 8000:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  vllm/vllm-openai:v0.12.0 \
  --model Qwen/Qwen2.5-7B-Instruct \
  --max-model-len 32768 \
  --gpu-memory-utilization 0.95 \
  --enable-prefix-caching \
  --trust-remote-code
```

#### æ­¥éª¤ 3: è¿›å…¥å®¹å™¨å¹¶æ›´æ–°ä¾èµ–

```bash
# è¿›å…¥å®¹å™¨
docker exec -it vllm-server bash

# æ›´æ–° transformers åˆ°æœ€æ–°é¢„è§ˆç‰ˆ
pip install -U transformers --pre

# éªŒè¯å®‰è£…
python -c "import transformers; print(transformers.__version__)"
```

#### æ­¥éª¤ 4: éªŒè¯æœåŠ¡

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æµ‹è¯•æ¨ç†
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "prompt": "Hello, how are you?",
    "max_tokens": 100
  }'
```

### æ–¹æ³• 2: Docker Compose éƒ¨ç½²

åˆ›å»º `docker-compose.vllm.yml`:

```yaml
version: '3.8'

services:
  vllm-server:
    image: vllm/vllm-openai:v0.12.0
    container_name: vllm-server
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      --model Qwen/Qwen2.5-7B-Instruct
      --max-model-len 32768
      --gpu-memory-utilization 0.95
      --enable-prefix-caching
      --trust-remote-code
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

å¯åŠ¨:

```bash
# å¯åŠ¨æœåŠ¡
docker-compose -f docker-compose.vllm.yml up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.vllm.yml logs -f

# åœæ­¢æœåŠ¡
docker-compose -f docker-compose.vllm.yml down
```

### æ–¹æ³• 3: åŸç”Ÿå®‰è£…

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv vllm-env
source vllm-env/bin/activate

# å®‰è£… vLLM
pip install vllm

# æ›´æ–° transformers
pip install -U transformers --pre

# å¯åŠ¨æœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --max-model-len 32768 \
  --gpu-memory-utilization 0.95
```

---

## âš™ï¸ æ¨¡å‹é…ç½®

### é…ç½®å‚æ•°è¯¦è§£

#### æ ¸å¿ƒå‚æ•°

| å‚æ•° | 24GB é…ç½® | 30GB+ é…ç½® | è¯´æ˜ |
|------|-----------|------------|------|
| `--max-model-len` | 12500-14000 | 32768 | æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ |
| `--gpu-memory-utilization` | 0.90 | 0.95 | GPU æ˜¾å­˜ä½¿ç”¨ç‡ |
| `--tensor-parallel-size` | 1 | 1-2 | å¼ é‡å¹¶è¡Œåº¦ï¼ˆå¤šå¡ï¼‰ |
| `--enable-prefix-caching` | false | true | å‰ç¼€ç¼“å­˜ |

#### æ€§èƒ½è°ƒä¼˜å‚æ•°

```bash
# æ‰¹å¤„ç†ä¼˜åŒ–
--max-num-batched-tokens 8192  # æ‰¹å¤„ç† token æ•°
--max-num-seqs 256  # æœ€å¤§å¹¶å‘åºåˆ—æ•°

# é‡åŒ–åŠ é€Ÿï¼ˆé™ä½æ˜¾å­˜å ç”¨ï¼‰
--quantization awq  # AWQ é‡åŒ–
--quantization gptq  # GPTQ é‡åŒ–

# KV ç¼“å­˜ä¼˜åŒ–
--block-size 16  # KV ç¼“å­˜å—å¤§å°
--swap-space 4  # CPU äº¤æ¢ç©ºé—´ (GB)
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# Hugging Face ç¼“å­˜è·¯å¾„
export HF_HOME=/path/to/cache
export TRANSFORMERS_CACHE=/path/to/cache

# CUDA å¯è§è®¾å¤‡
export CUDA_VISIBLE_DEVICES=0,1  # ä½¿ç”¨ GPU 0 å’Œ 1

# vLLM é…ç½®
export VLLM_LOGGING_LEVEL=INFO
export VLLM_USE_MODELSCOPE=false
```

### å¤šæ¨¡å‹é…ç½®

```yaml
# config/ai-models.yaml
models:
  - name: code-generation
    model_path: "codellama/CodeLlama-7b-Instruct-hf"
    max_len: 16384
    gpu_memory: 0.45
    
  - name: chat-model
    model_path: "Qwen/Qwen2.5-7B-Instruct"
    max_len: 32768
    gpu_memory: 0.45
```

---

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–

### æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥

#### 1. æ¨¡å‹é‡åŒ–

```bash
# AWQ é‡åŒ–ï¼ˆæ¨èï¼‰
--quantization awq
--model TheBloke/Llama-2-7B-AWQ

# GPTQ é‡åŒ–
--quantization gptq
--model TheBloke/Llama-2-7B-GPTQ
```

#### 2. åŠ¨æ€æ‰¹å¤„ç†

```bash
# å¯ç”¨åŠ¨æ€æ‰¹å¤„ç†
--max-num-batched-tokens 4096
--max-num-seqs 128
```

#### 3. å‰ç¼€ç¼“å­˜

```bash
# å¯ç”¨å‰ç¼€ç¼“å­˜ï¼ˆæ¨èé…ç½®ï¼‰
--enable-prefix-caching
```

### ååé‡ä¼˜åŒ–

```bash
# å¤š GPU å¼ é‡å¹¶è¡Œ
--tensor-parallel-size 2  # 2 å¡å¹¶è¡Œ

# Pipeline å¹¶è¡Œï¼ˆè¶…å¤§æ¨¡å‹ï¼‰
--pipeline-parallel-size 2

# å¢åŠ å·¥ä½œçº¿ç¨‹
--worker-use-ray
```

### å»¶è¿Ÿä¼˜åŒ–

```bash
# å‡å°‘æ‰¹å¤„ç†å»¶è¿Ÿ
--max-num-batched-tokens 2048
--max-num-seqs 64

# ä½¿ç”¨æµå¼è¾“å‡º
--stream-output
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. OOM (Out of Memory) é”™è¯¯

**ç—‡çŠ¶**:

```text
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ–¹æ¡ˆ 1: é™ä½ max-model-len
--max-model-len 12000

# æ–¹æ¡ˆ 2: é™ä½ GPU æ˜¾å­˜ä½¿ç”¨ç‡
--gpu-memory-utilization 0.85

# æ–¹æ¡ˆ 3: å‡å°‘æ‰¹å¤„ç†å¤§å°
--max-num-batched-tokens 2048

# æ–¹æ¡ˆ 4: ä½¿ç”¨é‡åŒ–æ¨¡å‹
--quantization awq
```

#### 2. CUDA ä¸å¯ç”¨

**ç—‡çŠ¶**:

```text
RuntimeError: No CUDA GPUs are available
```

**æ£€æŸ¥æ­¥éª¤**:

```bash
# 1. æ£€æŸ¥ NVIDIA é©±åŠ¨
nvidia-smi

# 2. æ£€æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version

# 3. éªŒè¯ PyTorch CUDA
python -c "import torch; print(torch.cuda.is_available())"

# 4. é‡æ–°å®‰è£… vLLMï¼ˆåŒ¹é… CUDA ç‰ˆæœ¬ï¼‰
pip uninstall vllm
pip install vllm-cuda118  # CUDA 11.8
# æˆ–
pip install vllm-cuda121  # CUDA 12.1
```

#### 3. æ¨¡å‹åŠ è½½å¤±è´¥

**ç—‡çŠ¶**:

```text
OSError: Can't load tokenizer for 'model_name'
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. æ›´æ–° transformers
pip install -U transformers --pre

# 2. æ¸…é™¤ç¼“å­˜
rm -rf ~/.cache/huggingface/*

# 3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
huggingface-cli download Qwen/Qwen2.5-7B-Instruct

# 4. ä½¿ç”¨æœ¬åœ°è·¯å¾„
--model /path/to/local/model
```

#### 4. æ€§èƒ½ä¸ä½³

**è¯Šæ–­å·¥å…·**:

```bash
# ç›‘æ§ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹ vLLM ç»Ÿè®¡
curl http://localhost:8000/metrics

# å‹åŠ›æµ‹è¯•
python benchmark.py \
  --model Qwen/Qwen2.5-7B-Instruct \
  --num-prompts 100 \
  --max-tokens 512
```

**ä¼˜åŒ–å»ºè®®**:

1. å¯ç”¨å‰ç¼€ç¼“å­˜: `--enable-prefix-caching`
2. è°ƒæ•´æ‰¹å¤„ç†å¤§å°
3. ä½¿ç”¨é‡åŒ–æ¨¡å‹
4. è€ƒè™‘å¤š GPU éƒ¨ç½²

#### 5. Docker å®¹å™¨æ— æ³•è®¿é—® GPU

**ç—‡çŠ¶**:

```text
docker: Error response from daemon: could not select device driver
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. å®‰è£… NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# 2. é‡å¯ Docker
sudo systemctl restart docker

# 3. æµ‹è¯• GPU è®¿é—®
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

### æ—¥å¿—å’Œç›‘æ§

#### å¯ç”¨è¯¦ç»†æ—¥å¿—

```bash
# vLLM æ—¥å¿—çº§åˆ«
export VLLM_LOGGING_LEVEL=DEBUG

# Python æ—¥å¿—
export PYTHONPATH=/app
python -m vllm.entrypoints.openai.api_server --log-level debug
```

#### ç›‘æ§ç«¯ç‚¹

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# Prometheus æŒ‡æ ‡
curl http://localhost:8000/metrics

# æ¨¡å‹ä¿¡æ¯
curl http://localhost:8000/v1/models
```

---

## ğŸ“š é›†æˆåˆ° SynergyMesh

### é…ç½®æ–‡ä»¶é›†æˆ

åœ¨ `config/ai-constitution.yaml` ä¸­æ·»åŠ :

```yaml
ai_models:
  llm:
    provider: vllm
    endpoint: http://localhost:8000
    model: Qwen/Qwen2.5-7B-Instruct
    max_tokens: 32768
    temperature: 0.7
    
  embedding:
    provider: sentence-transformers
    model: BAAI/bge-large-zh-v1.5
```

### æœåŠ¡å‘ç°

åœ¨ `core/unified_integration/` ä¸­æ³¨å†Œ AI æœåŠ¡:

```python
# core/unified_integration/ai_service_registry.py
from core.services.ai_client import VLLMClient

def register_ai_services():
    """æ³¨å†Œ AI æ¨¡å‹æœåŠ¡"""
    vllm_client = VLLMClient(
        base_url="http://localhost:8000",
        model="Qwen/Qwen2.5-7B-Instruct"
    )
    
    registry.register("ai.llm", vllm_client)
```

### å¥åº·æ£€æŸ¥é›†æˆ

åœ¨ `scripts/health-check.sh` ä¸­æ·»åŠ :

```bash
# æ£€æŸ¥ vLLM æœåŠ¡
check_vllm_health() {
  local url="http://localhost:8000/health"
  if curl -sf "$url" > /dev/null; then
    echo "âœ… vLLM service is healthy"
    return 0
  else
    echo "âŒ vLLM service is unhealthy"
    return 1
  fi
}
```

---

## ğŸ”— å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers/)

### ç›¸å…³æ–‡æ¡£

- [AI Module README](../ai/README.md)
- [éƒ¨ç½²æŒ‡å—](./DEPLOYMENT_GUIDE.md)
- [ç³»ç»Ÿæ¶æ„](./SYSTEM_ARCHITECTURE.md)

### ç¤¾åŒºæ”¯æŒ

- GitHub Issues: [vLLM Issues](https://github.com/vllm-project/vllm/issues)
- Discord: vLLM Community
- Hugging Face Forums

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-12-14)

- âœ… åˆå§‹ç‰ˆæœ¬
- âœ… æ·»åŠ ç¡¬ä»¶è¦æ±‚è¯´æ˜ï¼ˆ24GB æœ€ä½ / 30GB æ¨èï¼‰
- âœ… vLLM Docker éƒ¨ç½²æŒ‡å—
- âœ… é…ç½®å‚æ•°è¯¦è§£
- âœ… æ•…éšœæ’æŸ¥æŒ‡å—
- âœ… SynergyMesh é›†æˆè¯´æ˜

---

## ğŸ¤ è´¡çŒ®

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ– Pull Requestã€‚

**ç»´æŠ¤è€…**: SynergyMesh Team  
**è”ç³»æ–¹å¼**: [GitHub Issues](https://github.com/SynergyMesh-master/KeyStonOps/issues)
